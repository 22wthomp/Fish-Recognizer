{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing image crawler"
      ],
      "metadata": {
        "id": "I12QOtG5a5s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icrawler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IuI7chvV09Y",
        "outputId": "8280ad00-ce8e-4799-beda-48fe1ee03878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting icrawler\n",
            "  Downloading icrawler-0.6.10-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from icrawler) (4.13.4)\n",
            "Collecting bs4 (from icrawler)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from icrawler) (5.3.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from icrawler) (11.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from icrawler) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from icrawler) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from icrawler) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->icrawler) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->icrawler) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (2025.1.31)\n",
            "Downloading icrawler-0.6.10-py3-none-any.whl (36 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4, icrawler\n",
            "Successfully installed bs4-0.0.2 icrawler-0.6.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run ONLY if you have phantom folders in the colab directory"
      ],
      "metadata": {
        "id": "KoKf79Wka_7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Try unmounting (ignore error if any)\n",
        "!fusermount -u /content/drive\n",
        "\n",
        "# Step 2: Delete ghost mount folder\n",
        "!rm -rf /content/drive\n",
        "\n",
        "# Step 3: Now mount cleanly\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BciwaQ3Gx5V0",
        "outputId": "d014d775-636b-47d3-b0b3-99b2f58369b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: Invalid argument\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "HneKcxiuy3gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UG7I5PAQbPXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Start of Data Processing**"
      ],
      "metadata": {
        "id": "0GT6eVlYbbaZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AVQhfQIQMNiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6848997f-bc07-4df2-ac42-5c9fd77a932b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial Crawler to grab images of fish"
      ],
      "metadata": {
        "id": "MLUbAs_CbkjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from icrawler.builtin import GoogleImageCrawler\n",
        "import os\n",
        "\n",
        "# Folder in your Google Drive where images will be saved\n",
        "base_path = '/content/drive/MyDrive/Ai1/Project/fish_dataset'\n",
        "\n",
        "# List of fish species to scrape\n",
        "species_list = ['betta fish', 'guppy fish', 'neon tetra', 'goldfish']\n",
        "\n",
        "for species in species_list:\n",
        "    species_folder = os.path.join(base_path, species.replace(\" \", \"_\"))\n",
        "    os.makedirs(species_folder, exist_ok=True)\n",
        "\n",
        "    crawler = GoogleImageCrawler(storage={'root_dir': species_folder})\n",
        "    crawler.crawl(keyword=species + \" aquarium\", max_num=100)  # Adjust number as needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ppDc10JzWGXb",
        "outputId": "0bd67607-c364-4449-c085-cc3473fecd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'icrawler'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-72df64256cbb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0micrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleImageCrawler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Folder in your Google Drive where images will be saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Ai1/Project/fish_dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'icrawler'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get more images of each fish creating a crawler specifically for betta fish, trying different key words that might improve the results from intial crawler"
      ],
      "metadata": {
        "id": "RwET5H62bvYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from icrawler.builtin import BingImageCrawler\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "import os\n",
        "species = 'betta fish'\n",
        "keywords = ['betta fish', 'betta fish aquarium', 'betta fish closeup','cool betta fish','Siamese fighting fish','Siamese fighting fish close up','Vietnamese fighting fish','Vietnamese fighting fish close up']\n",
        "base_path = '/content/drive/MyDrive/AI1/Project/fish_dataset'\n",
        "folder = os.path.join(base_path, species.replace(\" \", \"_\"))\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "crawler = GoogleImageCrawler(storage={'root_dir': folder})\n",
        "\n",
        "for keyword in keywords:\n",
        "    print(f\"Scraping: {keyword}\")\n",
        "    crawler.crawl(keyword=keyword, max_num=30)\n",
        "crawler.crawl(keyword='betta fish side view', max_num=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJSJcR0WbGfQ",
        "outputId": "129d9b96-a111-4dcd-8184-d3544b3cce5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: betta fish\n",
            "Scraping: betta fish aquarium\n",
            "Scraping: betta fish closeup\n",
            "Scraping: cool betta fish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 404, file https://t1.pixers.pics/img-d5043af1/canvas-prints-colorful-betta-fish-closeup-dragon-fish-aquarium.png\n",
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/which-colour-of-betta-fish-do-you-think-is-coolest-v0-26wz9x3o3x8c1.jpeg?width=4032&format=pjpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: Siamese fighting fish\n",
            "Scraping: Siamese fighting fish close up\n",
            "Scraping: Vietnamese fighting fish\n",
            "Scraping: Vietnamese fighting fish close up\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/Betta_splendens_male_crowntail.png\n",
            "ERROR:downloader:Response status code 404, file https://t1.pixers.pics/img-d5043af1/canvas-prints-side-view-of-a-siamese-fighting-fish-betta-splendens-isolated.png\n",
            "ERROR:downloader:Response status code 400, file https://img.pixers.pics/download(cms:2023/10/652913452195f_fototapety-salon-mockup.png\n",
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/1201981412/photo/close-up-art-movement-of-betta-fish-siamese-fighting-fish-isolated-on-black-background-fine.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/1417273996/photo/blue-fighting-fish-isolated-on-white-background-this-has-clipping-path.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crawler for goldfish (Gold fish was removed from the final dataset due to there being many different kinds of goldfish that by looking at them have nothing in common as well as it being a schooling fish that is rarely photographed by itself)"
      ],
      "metadata": {
        "id": "9nKssWSCcNq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "species = 'goldfish'\n",
        "keywords = ['goldfish side profile HD','goldfish alone', 'goldfish single goldfish solo tank','red goldfish HD','goldfish full body','goldfish solo','lonley goldfish','single goldfish','goldfish  alone stock white background','solo goldfish side profile']\n",
        "base_path = '/content/drive/MyDrive/AI1/Project/fish_dataset'\n",
        "folder = os.path.join(base_path, species.replace(\" \", \"_\"))\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "crawler = BingImageCrawler(storage={'root_dir': folder})\n",
        "\n",
        "for keyword in keywords:\n",
        "    print(f\"Scraping: {keyword}\")\n",
        "    crawler.crawl(keyword=keyword, max_num=30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRkbCUOq_MMo",
        "outputId": "caddb433-45b9-4de3-e3cd-6aa06e7231cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: goldfish side profile HD\n",
            "Scraping: goldfish alone\n",
            "Scraping: goldfish single goldfish solo tank\n",
            "Scraping: red goldfish HD\n",
            "Scraping: goldfish full body\n",
            "Scraping: goldfish solo\n",
            "Scraping: lonley goldfish\n",
            "Scraping: single goldfish\n",
            "Scraping: goldfish  alone stock white background\n",
            "Scraping: solo goldfish side profile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crawler for guppy"
      ],
      "metadata": {
        "id": "FSTxBdAxcpIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "species = 'guppy_fish'\n",
        "keywords = ['guppy fish side view','guppy fish full body','guppy fish swimming left','single guppy fish in tank','guppy fish alone','guppy solo aquarium','guppy fish HD','guppy fish macro photo','guppy fish close-up','red guppy fish side view',\n",
        "'blue guppy full body','Poecilia reticulata aquarium','Poecilia reticulata side profile','guppy fish','guppy full body']\n",
        "base_path = '/content/drive/MyDrive/AI1/Project/fish_dataset'\n",
        "folder = os.path.join(base_path, species.replace(\" \", \"_\"))\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "crawler = GoogleImageCrawler(storage={'root_dir': folder})\n",
        "\n",
        "for keyword in keywords:\n",
        "    print(f\"Scraping: {keyword}\")\n",
        "    crawler.crawl(keyword=keyword, max_num=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gw8XNhBH1pv",
        "outputId": "7acdd6cd-f448-49ca-aacc-491dfb552649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: guppy fish side view\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.mdpi.com/sensors/sensors-22-05545/article_deploy/html/images/sensors-22-05545-g001.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: guppy fish full body\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 404, file https://www.navyaquarium.in/wp-content/uploads/2022/08/pregnant-guppy-fish-died-after-giving-birth-PhotoRoom.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: guppy fish swimming left\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Exception caught when downloading file https%3A%2F%2Fimages.unsplash.com%2Fopengraph%2Flogo.png, error: '', remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https%3A%2F%2Fimages.unsplash.com%2Fopengraph%2Flogo.png, error: '', remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https%3A%2F%2Fimages.unsplash.com%2Fopengraph%2Flogo.png, error: '', remaining retry times: 0\n",
            "ERROR:downloader:Exception caught when downloading file https%3A%2F%2Fimages.unsplash.com%2Fphoto-1711826949944-78aa8b7be563%3Fcrop%3Dfaces%252Cedges%26h%3D630%26w%3D1200%26blend%3D000000%26blend-mode%3Dnormal%26blend-alpha%3D10%26mark-w%3D750%26mark-align%3Dmiddle%252Ccenter%26mark%3Dhttps%253A%252F%252Fimages.unsplash.com%252Fopengraph%252Fsearch-input.png, error: '', remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https%3A%2F%2Fimages.unsplash.com%2Fphoto-1711826949944-78aa8b7be563%3Fcrop%3Dfaces%252Cedges%26h%3D630%26w%3D1200%26blend%3D000000%26blend-mode%3Dnormal%26blend-alpha%3D10%26mark-w%3D750%26mark-align%3Dmiddle%252Ccenter%26mark%3Dhttps%253A%252F%252Fimages.unsplash.com%252Fopengraph%252Fsearch-input.png, error: '', remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https%3A%2F%2Fimages.unsplash.com%2Fphoto-1711826949944-78aa8b7be563%3Fcrop%3Dfaces%252Cedges%26h%3D630%26w%3D1200%26blend%3D000000%26blend-mode%3Dnormal%26blend-alpha%3D10%26mark-w%3D750%26mark-align%3Dmiddle%252Ccenter%26mark%3Dhttps%253A%252F%252Fimages.unsplash.com%252Fopengraph%252Fsearch-input.png, error: '', remaining retry times: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: single guppy fish in tank\n",
            "Scraping: guppy fish alone\n",
            "Scraping: guppy solo aquarium\n",
            "Scraping: guppy fish HD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/i-have-been-planning-on-getting-a-10-gallon-tank-for-v0-8lctpwt7ce8b1.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: guppy fish macro photo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 304, file https://glassboxdiaries.com/wp-content/uploads/2024/01/1.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: guppy fish close-up\n",
            "Scraping: red guppy fish side view\n",
            "Scraping: blue guppy full body\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 404, file https://res.cloudinary.com/jerrick/image/upload/d_642250b563292b35f27461a7.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: Poecilia reticulata aquarium\n",
            "Scraping: Poecilia reticulata side profile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.researchgate.net/publication/321036721/figure/fig1/AS:570269774422016@1512974317291/Adults-of-non-native-guppies-Poecilia-reticulata-females-358-mm-SL-male-247-mm.png\n",
            "ERROR:downloader:Response status code 403, file https://www.researchgate.net/publication/336486726/figure/fig1/AS:813465358843905@1570956663657/Poecilia-reticulata-male-above-and-female-below-from-the-Namak-Lake-basin-Iran.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: guppy fish\n",
            "Scraping: guppy full body\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crawler for Neon tetra (This species was removed due to being a small fast schooling fish, most images of this species where blury specs of color, Did not follow the image assumption of 1 fish per image)"
      ],
      "metadata": {
        "id": "stY9ye24criv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "species = 'neon_tetra'\n",
        "keywords = ['neon tetra side view','neon tetra full body fish','neon tetra swimming left','single neon tetra fish','neon tetra alone in tank','solo neon tetra aquarium','neon tetra fish HD',\n",
        "'neon tetra macro photography','neon tetra close-up photo','Paracheirodon innesi side view','Paracheirodon innesi solo fish','neon tetra alone','single neon tetra white background']\n",
        "base_path = '/content/drive/MyDrive/AI1/Project/fish_dataset'\n",
        "folder = os.path.join(base_path, species.replace(\" \", \"_\"))\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "crawler = BingImageCrawler(storage={'root_dir': folder})\n",
        "\n",
        "for keyword in keywords:\n",
        "    print(f\"Scraping: {keyword}\")\n",
        "    crawler.crawl(keyword=keyword, max_num=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W16Rh09VN4RY",
        "outputId": "c0c3ed4e-c6d9-45d7-ebae-117046cf232a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: neon tetra side view\n",
            "Scraping: neon tetra full body fish\n",
            "Scraping: neon tetra swimming left\n",
            "Scraping: single neon tetra fish\n",
            "Scraping: neon tetra alone in tank\n",
            "Scraping: solo neon tetra aquarium\n",
            "Scraping: neon tetra fish HD\n",
            "Scraping: neon tetra macro photography\n",
            "Scraping: neon tetra close-up photo\n",
            "Scraping: Paracheirodon innesi side view\n",
            "Scraping: Paracheirodon innesi solo fish\n",
            "Scraping: neon tetra alone\n",
            "Scraping: single neon tetra white background\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gourami crawler"
      ],
      "metadata": {
        "id": "3QlDzdkFdAHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "species = 'gourami'\n",
        "keywords = ['Gourami (Dwarf, Pearl)','dwarf gourami','gourami solo','dwarf gourami solo','pearl gourami solo','red gourami solo side view','gourami side view','flame gourami solo','flame gourami side profile solo','powder blue gourami solo','powder blue gourami side profile','honey gourami solo','honey gourami side profile','honey sunset gourami side profile solo','opaline gourami side profile','opaline gourami side profile' ]\n",
        "base_path = '/content/drive/MyDrive/AI1/Project/fish_dataset'\n",
        "folder = os.path.join(base_path, species.replace(\" \", \"_\"))\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "crawler = GoogleImageCrawler(storage={'root_dir': folder})\n",
        "\n",
        "for keyword in keywords:\n",
        "    print(f\"Scraping: {keyword}\")\n",
        "    crawler.crawl(keyword=keyword, max_num=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA1Ax-ioQUNe",
        "outputId": "fdacfc5c-289e-499f-b28c-b2b74cbb0ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: Gourami (Dwarf, Pearl)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.seahorseaquariums.com/image/cache/catalog/Categories%20-%20Freshewater/Freshwater%20Fish%20Menu/Pearl%20Gourami-634x634.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: dwarf gourami\n",
            "Scraping: gourami solo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://external-preview.redd.it/solo-gourami-suddenly-going-bonkers-at-feeding-time-v0-c3dtZjQ5dnE3ZnZjMSXNLuh_UJdiYkCVmNmgNZU60J_DzVkhYA-NLgjOni7X.png\n",
            "ERROR:downloader:Response status code 403, file https://i.redd.it/saved-a-one-eyed-honey-gourami-from-petco-v0-g1s7pa5z5d8e1.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: dwarf gourami solo\n",
            "Scraping: pearl gourami solo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/female-pearl-gourami-alone-v0-zf4rx9nc5lpa1.jpeg?width=1372&format=pjpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: red gourami solo side view\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/what-are-your-thoughts-on-honey-gouramis-v0-az2z09z7d3gd1.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: gourami side view\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.researchgate.net/publication/299598774/figure/fig3/AS:613869714173981@1523369353565/Landmarks-used-for-digitizing-image-of-Three-spotted-Gourami-Trichopodus-trichopterus.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: flame gourami solo\n",
            "Scraping: flame gourami side profile solo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 401, file https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/7a9e0e2d-6a18-4258-9351-39be3d35bce6/d96157c-a2841693-97ea-497e-a179-158032446f76.png\n",
            "ERROR:downloader:Response status code 403, file https://external-preview.redd.it/-5DHl7Vvf04Uxe_lGlLzC6Xc-MqHXur1tjM_0i6eTCQ.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: powder blue gourami solo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/powder-blue-dwarf-gourami-v0-smb2q4067xuc1.jpeg?width=4032&format=pjpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: powder blue gourami side profile\n",
            "Scraping: honey gourami solo\n",
            "Scraping: honey gourami side profile\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://media.invisioncic.com/b300999/monthly_2023_03/image.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: honey sunset gourami side profile solo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/saved-a-one-eyed-honey-gourami-from-petco-v0-2y5zs8dsrh8e1.jpeg?width=4320&format=pjpg\n",
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/sunset-honey-female-turning-dark-v0-0wmx2ynbwcuc1.jpeg?width=3000&format=pjpg\n",
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/saved-a-one-eyed-honey-gourami-from-petco-v0-z20nfix2sh8e1.jpeg?width=2252&format=pjpg\n",
            "ERROR:downloader:Response status code 403, file https://preview.redd.it/sunset-honey-or-thick-lipped-v0-bme3pw9lgywd1.jpeg?width=3024&format=pjpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: opaline gourami side profile\n",
            "Scraping: opaline gourami side profile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "killifish crawler"
      ],
      "metadata": {
        "id": "dtNl7a1QdDcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from icrawler.builtin import BingImageCrawler\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "import os\n",
        "species = 'killifish'\n",
        "keywords = [\n",
        "    'killifish side view',\n",
        "    'killifish full body aquarium photo',\n",
        "    'single killifish swimming left',\n",
        "    'killifish macro photography',\n",
        "    'golden wonder killifish HD',\n",
        "    'killifish solo in planted tank',\n",
        "    'australe killifish side profile',\n",
        "    'killifish close up aquarium',\n",
        "    'nothobranchius killifish full body',\n",
        "    'killifish clear water image'\n",
        "]\n",
        "base_path = '/content/drive/MyDrive/fish_dataset_copy/'\n",
        "folder = os.path.join(base_path, species.replace(\" \", \"_\"))\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "crawler = BingImageCrawler(storage={'root_dir': folder})\n",
        "\n",
        "for keyword in keywords:\n",
        "    print(f\"Scraping: {keyword}\")\n",
        "    crawler.crawl(keyword=keyword, max_num=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4IPAEWmZuDh",
        "outputId": "6f6d7804-3337-4c84-ea4a-50158bd08d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: killifish side view\n",
            "Scraping: killifish full body aquarium photo\n",
            "Scraping: single killifish swimming left\n",
            "Scraping: killifish macro photography\n",
            "Scraping: golden wonder killifish HD\n",
            "Scraping: killifish solo in planted tank\n",
            "Scraping: australe killifish side profile\n",
            "Scraping: killifish close up aquarium\n",
            "Scraping: nothobranchius killifish full body\n",
            "Scraping: killifish clear water image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['imageURL'].isna()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pZyF0FqNBn3",
        "outputId": "11863fa7-7c78-4c7d-84cc-8b4d0e5b352b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(100)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code counts the number of images in each folder"
      ],
      "metadata": {
        "id": "iVgIY6vIdOW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/fish_dataset_copy/' #base directory\n",
        "\n",
        "# list of folders contain fish images\n",
        "fish_folders = [\n",
        "    'betta_fish_oriented',\n",
        "    'killifish_oriented',\n",
        "    'gourami_oriented',\n",
        "    'guppy_fish_oriented'\n",
        "]\n",
        "\n",
        "for folder in fish_folders:\n",
        "    full_path = os.path.join(base_dir, folder)\n",
        "\n",
        "    if not os.path.exists(full_path):\n",
        "        print(f\"Folder not found: {folder}\")\n",
        "        continue\n",
        "\n",
        "    image_count = len([\n",
        "        f for f in os.listdir(full_path)\n",
        "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "    ])\n",
        "\n",
        "    print(f\"{folder}: {image_count} image(s)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvO1AjQofrAc",
        "outputId": "206e08a4-b87c-4323-c58b-37d2dc9cd353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "betta_fish_oriented: 89 image(s)\n",
            "killifish_oriented: 77 image(s)\n",
            "gourami_oriented: 106 image(s)\n",
            "guppy_fish_oriented: 111 image(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Renames files to remove duplicate names"
      ],
      "metadata": {
        "id": "hoKSbaxXdrdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def safe_rename_all_images(base_path, folders):\n",
        "    for folder in folders:\n",
        "        folder_path = os.path.join(base_path, folder)\n",
        "        files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        # rename images to temp name\n",
        "        for i, fname in enumerate(sorted(files)):\n",
        "            ext = os.path.splitext(fname)[1].lower()\n",
        "            temp_name = f\"temp_{i:04d}{ext}\"\n",
        "            src = os.path.join(folder_path, fname)\n",
        "            dst = os.path.join(folder_path, temp_name)\n",
        "            os.rename(src, dst)\n",
        "\n",
        "        # renames temp names\n",
        "        temp_files = [f for f in os.listdir(folder_path) if f.startswith('temp_')]\n",
        "\n",
        "        for i, temp_fname in enumerate(sorted(temp_files)):\n",
        "            ext = os.path.splitext(temp_fname)[1].lower()\n",
        "            new_name = f\"{folder.replace('_oriented', '')}_{str(i+1).zfill(3)}{ext}\"\n",
        "            src = os.path.join(folder_path, temp_fname)\n",
        "            dst = os.path.join(folder_path, new_name)\n",
        "            os.rename(src, dst)\n",
        "\n",
        "        print(f\"Safely renamed files in {folder}\")\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/fish_dataset_copy3'\n",
        "oriented_folders = [\n",
        "    'betta_fish_oriented',\n",
        "    'killifish_oriented',\n",
        "    'gourami_oriented',\n",
        "    'guppy_fish_oriented'\n",
        "]\n",
        "\n",
        "safe_rename_all_images(dataset_path, oriented_folders)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBY0H8FAjUBu",
        "outputId": "cfb1245c-22a4-4fbb-dd41-0830d1f7b1a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Safely renamed files in betta_fish_oriented\n",
            "Safely renamed files in killifish_oriented\n",
            "Safely renamed files in gourami_oriented\n",
            "Safely renamed files in guppy_fish_oriented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removes duplicate images"
      ],
      "metadata": {
        "id": "IfVIPhgleLa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "keeps a dict of seen images as a hash if it sees the same ones it is a duplicate and removes the duplicate image"
      ],
      "metadata": {
        "id": "GhzGhUh7eZr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def remove_duplicates(base_path, folders):\n",
        "    seen_hashes = {}\n",
        "    removed_files = []\n",
        "\n",
        "    for folder in folders:\n",
        "        folder_path = os.path.join(base_path, folder)\n",
        "        files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        for fname in files:\n",
        "            fpath = os.path.join(folder_path, fname)\n",
        "            with open(fpath, 'rb') as f:\n",
        "                file_hash = hashlib.md5(f.read()).hexdigest()\n",
        "\n",
        "            if file_hash in seen_hashes:\n",
        "                os.remove(fpath)\n",
        "                removed_files.append(fpath)\n",
        "            else:\n",
        "                seen_hashes[file_hash] = fpath\n",
        "\n",
        "    print(f\"Removed {len(removed_files)} duplicates.\")\n",
        "    return removed_files\n",
        "\n",
        "removed_files = remove_duplicates(base_path, oriented_folders)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z4_NaZnjykB",
        "outputId": "7d2d1b52-fde6-4282-bf0c-ba1d9e7f80a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 43 duplicates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check number of images after removing the duplicates"
      ],
      "metadata": {
        "id": "8kBn4v_qexg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/fish_dataset_copy/'\n",
        "\n",
        "fish_folders = [\n",
        "    'betta_fish_oriented',\n",
        "    'killifish_oriented',\n",
        "    'gourami_oriented',\n",
        "    'guppy_fish_oriented'\n",
        "]\n",
        "\n",
        "for folder in fish_folders:\n",
        "    full_path = os.path.join(base_dir, folder)\n",
        "\n",
        "    if not os.path.exists(full_path):\n",
        "        print(f\"Folder not found: {folder}\")\n",
        "        continue\n",
        "\n",
        "    image_count = len([\n",
        "        f for f in os.listdir(full_path)\n",
        "        if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "    ])\n",
        "\n",
        "    print(f\"{folder}: {image_count} image(s)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVNOLEm6kiV9",
        "outputId": "c8bb84b0-4d15-48b1-8738-e5c26f3ab573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "betta_fish_oriented: 80 image(s)\n",
            "killifish_oriented: 77 image(s)\n",
            "gourami_oriented: 89 image(s)\n",
            "guppy_fish_oriented: 72 image(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates Back up of folder"
      ],
      "metadata": {
        "id": "1I4DclNQe-RF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# original dataset path\n",
        "src = '/content/drive/MyDrive/fish_dataset_copy/_cropped_clean2'\n",
        "\n",
        "# backup copy path\n",
        "dst = '/content/drive/MyDrive/fish_dataset_copy3'\n",
        "\n",
        "# If the backup folder already exists\n",
        "if os.path.exists(dst):\n",
        "    print(\"Backup folder already exists — skipping copy.\")\n",
        "else:\n",
        "    shutil.copytree(src, dst)\n",
        "    print(\"Backup created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUDJ123iZJ5b",
        "outputId": "155b17be-3181-4d06-8342-6a528281b5e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backup created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flips images to all face left, then flips 20 percent back"
      ],
      "metadata": {
        "id": "rge7r6YtfNGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#flip images\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/fish_dataset_copy3/killifish' # ran manually on each folder of fish to make sure it was done correctly\n",
        "output_folder = input_folder + '_oriented'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "variation_percent = 0.2 # % of left-facing fish to flip back\n",
        "variation_pool = []\n",
        "\n",
        "def is_facing_right(img_path):\n",
        "    #Heuristically determine if fish is facing right.\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        return False\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    edges = cv2.Canny(blurred, 50, 150)\n",
        "\n",
        "    h, w = edges.shape\n",
        "    left_half = edges[:, :w // 2]\n",
        "    right_half = edges[:, w // 2:]\n",
        "\n",
        "    left_score = np.sum(left_half)\n",
        "    right_score = np.sum(right_half)\n",
        "\n",
        "    return right_score > left_score\n",
        "\n",
        "# detects and flips right facing fish\n",
        "for filename in os.listdir(input_folder):\n",
        "    if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        continue\n",
        "\n",
        "    full_path = os.path.join(input_folder, filename)\n",
        "    save_path = os.path.join(output_folder, filename)\n",
        "\n",
        "    img = Image.open(full_path)\n",
        "\n",
        "    if is_facing_right(full_path):\n",
        "        img = img.transpose(Image.FLIP_LEFT_RIGHT)  # Flip to face left\n",
        "        print(f\"Flipped to left: {filename}\")\n",
        "    else:\n",
        "        # mark it for possible flip-back later\n",
        "        variation_pool.append((img, save_path))\n",
        "        print(f\"Already left: {filename}\")\n",
        "\n",
        "    img.save(save_path)\n",
        "#flip back 20% of already-left-facing fish\n",
        "num_to_flip_back = int(len(variation_pool) * variation_percent)\n",
        "random.shuffle(variation_pool)\n",
        "\n",
        "for img, path in variation_pool[:num_to_flip_back]:\n",
        "    flipped_img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    flipped_img.save(path)\n",
        "    print(f\"Flipped BACK to right: {os.path.basename(path)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwmvnHkAT2J_",
        "outputId": "06093039-673d-4222-bed8-914497114eed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flipped to left: killifish_001.jpg\n",
            "Flipped to left: killifish_002.jpg\n",
            "Flipped to left: killifish_003.png\n",
            "Flipped to left: killifish_005.png\n",
            "Already left: killifish_006.jpg\n",
            "Already left: killifish_009.png\n",
            "Already left: killifish_012.png\n",
            "Already left: killifish_024.png\n",
            "Flipped to left: killifish_025.jpg\n",
            "Flipped to left: killifish_030.png\n",
            "Flipped to left: killifish_034.jpg\n",
            "Already left: killifish_037.jpg\n",
            "Flipped to left: killifish_038.png\n",
            "Already left: killifish_040.jpg\n",
            "Flipped to left: killifish_041.png\n",
            "Already left: killifish_042.png\n",
            "Already left: killifish_043.jpg\n",
            "Flipped to left: killifish_044.jpg\n",
            "Already left: killifish_045.png\n",
            "Already left: killifish_046.jpg\n",
            "Flipped to left: killifish_047.jpg\n",
            "Already left: killifish_050.png\n",
            "Flipped to left: killifish_051.png\n",
            "Already left: killifish_011.png\n",
            "Already left: killifish_022.jpg\n",
            "Already left: killifish_010.jpg\n",
            "Already left: killifish_008.jpg\n",
            "Flipped to left: killifish_048.jpg\n",
            "Already left: killifish_021.jpg\n",
            "Flipped to left: killifish_023.jpg\n",
            "Already left: killifish_016.png\n",
            "Already left: killifish_007.png\n",
            "Already left: killifish_033.jpg\n",
            "Already left: killifish_017.jpg\n",
            "Already left: killifish_015.jpg\n",
            "Already left: killifish_013.png\n",
            "Already left: killifish_018.jpg\n",
            "Already left: killifish_049.jpg\n",
            "Already left: killifish_004.jpg\n",
            "Already left: killifish_031.png\n",
            "Already left: killifish_014.jpg\n",
            "Already left: killifish_032.jpg\n",
            "Already left: killifish_019.jpg\n",
            "Already left: killifish_035.jpg\n",
            "Already left: killifish_020.png\n",
            "Already left: killifish_036.jpg\n",
            "Already left: killifish_026.jpg\n",
            "Already left: killifish_039.jpg\n",
            "Flipped to left: killifish_027.png\n",
            "Already left: killifish_028.jpg\n",
            "Flipped to left: killifish_029.jpg\n",
            "Flipped BACK to right: killifish_007.png\n",
            "Flipped BACK to right: killifish_046.jpg\n",
            "Flipped BACK to right: killifish_040.jpg\n",
            "Flipped BACK to right: killifish_017.jpg\n",
            "Flipped BACK to right: killifish_014.jpg\n",
            "Flipped BACK to right: killifish_036.jpg\n",
            "Flipped BACK to right: killifish_050.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loops through folders given finds subject crops around it or adds padding to condense/resize image to 244 by 244 px"
      ],
      "metadata": {
        "id": "feP8893SgPhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "#basePath = '/content/drive/MyDrive/fish_dataset_copy/' #base path for crawler images\n",
        "basePath = '/content/drive/MyDrive/AI1/Project/Unsorted_fish/' #base path for images gathered at petco\n",
        "base_output_path = basePath + '_cropped_clean2'\n",
        "'''fish_folders = [ #folders for crawler images\n",
        "    'betta_fish_oriented',\n",
        "    'killifish_oriented',\n",
        "    'gourami_oriented',\n",
        "    'guppy_fish_oriented'\n",
        "]'''\n",
        "fish_folders = [ #folders for images gathered from petco\n",
        "    'bettas',\n",
        "    'Gourami',\n",
        "    'Guppies'\n",
        "]\n",
        "\n",
        "os.makedirs(base_output_path, exist_ok=True)\n",
        "\n",
        "def resize_and_pad(img, size=(224, 224)):\n",
        "    h, w = img.shape[:2]\n",
        "    scale = min(size[0] / h, size[1] / w)\n",
        "    nh, nw = int(h * scale), int(w * scale)\n",
        "    img_resized = cv2.resize(img, (nw, nh))\n",
        "\n",
        "    # Create new image and center the resized image inside\n",
        "    result = np.zeros((size[0], size[1], 3), dtype=np.uint8)\n",
        "    y_offset = (size[0] - nh) // 2\n",
        "    x_offset = (size[1] - nw) // 2\n",
        "    result[y_offset:y_offset+nh, x_offset:x_offset+nw] = img_resized\n",
        "\n",
        "    return result\n",
        "\n",
        "for fish_folder in fish_folders:\n",
        "    input_folder = os.path.join(basePath, fish_folder)\n",
        "    output_folder = os.path.join(base_output_path, fish_folder)\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            continue\n",
        "\n",
        "        path = os.path.join(input_folder, filename)\n",
        "        img = cv2.imread(path)\n",
        "\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "        _, thresh = cv2.threshold(blurred, 50, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if not contours:\n",
        "            continue\n",
        "\n",
        "        best_contour = max(contours, key=cv2.contourArea)\n",
        "        x, y, w, h = cv2.boundingRect(best_contour)\n",
        "\n",
        "        if w < 50 or h < 50 or w/h > 3 or h/w > 3:\n",
        "            continue\n",
        "\n",
        "        pad = int(max(w, h) * 0.2)\n",
        "        x1 = max(0, x - pad)\n",
        "        y1 = max(0, y - pad)\n",
        "        x2 = min(img.shape[1], x + w + pad)\n",
        "        y2 = min(img.shape[0], y + h + pad)\n",
        "\n",
        "        cropped = img[y1:y2, x1:x2]\n",
        "        resized = resize_and_pad(cropped)\n",
        "\n",
        "        cv2.imwrite(os.path.join(output_folder, filename), resized)\n",
        "\n",
        "    print(f\"Finished processing {fish_folder}\")"
      ],
      "metadata": {
        "id": "KcjdpmphmqFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03ec720-b90f-43f2-e676-8c4bb841d0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Finished processing bettas\n",
            "✅ Finished processing Gourami\n",
            "✅ Finished processing Guppies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "merging petco images and crawler images where manually done as well as double checking ALL images for how clean they where"
      ],
      "metadata": {
        "id": "5SlDaYu8hDWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting merged fish images into a test, training and validation set"
      ],
      "metadata": {
        "id": "EWHupZJxhQ_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# input folder : the final clean dataset\n",
        "input_folder = '/content/drive/MyDrive/fish_dataset_copy/_cropped_clean2'\n",
        "\n",
        "# Output base folder for split dataset\n",
        "output_base = '/content/drive/MyDrive/fish_dataset_copy/final_split'\n",
        "splits = ['train', 'val', 'test']\n",
        "\n",
        "# folders containg fish\n",
        "species_folders = ['betta_fish_oriented', 'killifish_oriented', 'gourami_oriented', 'guppy_fish_oriented']\n",
        "\n",
        "# creates output folders\n",
        "for split in splits:\n",
        "    for species in species_folders:\n",
        "        os.makedirs(os.path.join(output_base, split, species), exist_ok=True)\n",
        "\n",
        "#split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# split\n",
        "for species in species_folders:\n",
        "    species_path = os.path.join(input_folder, species)\n",
        "    images = [f for f in os.listdir(species_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    random.shuffle(images)\n",
        "\n",
        "    n_total = len(images)\n",
        "    n_train = int(n_total * train_ratio)\n",
        "    n_val = int(n_total * val_ratio)\n",
        "\n",
        "    train_files = images[:n_train]\n",
        "    val_files = images[n_train:n_train+n_val]\n",
        "    test_files = images[n_train+n_val:]\n",
        "\n",
        "    for img_file in train_files:\n",
        "        shutil.copy(os.path.join(species_path, img_file),\n",
        "                    os.path.join(output_base, 'train', species, img_file))\n",
        "\n",
        "    for img_file in val_files:\n",
        "        shutil.copy(os.path.join(species_path, img_file),\n",
        "                    os.path.join(output_base, 'val', species, img_file))\n",
        "\n",
        "    for img_file in test_files:\n",
        "        shutil.copy(os.path.join(species_path, img_file),\n",
        "                    os.path.join(output_base, 'test', species, img_file))\n",
        "\n",
        "print(\"Finished splitting into train/val/test!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBk9OavRSVcK",
        "outputId": "bb11b71f-2886-4b59-b1bf-7f59005cd23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Finished splitting into train/val/test!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Now ready for model fitting!!**"
      ],
      "metadata": {
        "id": "aGtmCyLlhah6"
      }
    }
  ]
}